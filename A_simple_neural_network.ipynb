{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple neural network\n",
    "\n",
    "#### Curcuraci Luca\n",
    "\n",
    "#### 26/02/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "In this notebook we will present how to constrct a very simple neural nework able to classify the handwritten digit of the famous MNIST database. The code is written in Python3 and is essentially based on the Numpy library. With this work, we will see how a neural network can be realized in practise, which are its basic building blocks and how they works.\n",
    "\n",
    "##### Sources and releted material\n",
    "\n",
    "This notebook is essentially based on the book\n",
    "\n",
    "* M. Nielsen - Neural Networks and Deep Learning (http://neuralnetworksanddeeplearning.com/index.html)\n",
    "\n",
    "from which all the essential parts of the code described here are taken. For more detailed information, it can be also useful to consult\n",
    "\n",
    "* https://web.archive.org/web/20170101015004/http://www.deeplearning.net/tutorial/contents.html#,\n",
    "* http://ufldl.stanford.edu/tutorial/.\n",
    "\n",
    "The MNIST database can be downoload at the following link\n",
    "\n",
    "* https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz\n",
    "\n",
    "while info on this database (in particular the performance of vairous classification methods) are available in\n",
    "\n",
    "* http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "##### Disclaimer\n",
    "\n",
    "Any possible mistake or misunderstaing in the explanation of the  code is not intentional. We invite the reader to consult the sources above for clarifications. In the present notebook, possible typos or grammar mistakes can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list now the full code which is able to upload and process the MNIST database, print a sample of this database, and construct the neural network (nn for short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import _pickle as pkl\n",
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Functions\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    file = gzip.open('mnist.pkl.gz','rb')\n",
    "    train_set, valid_set, test_set = pkl.load(file,encoding='latin1')\n",
    "    file.close()\n",
    "    return(train_set, valid_set, test_set)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \n",
    "    print('Loading database...')\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    print ('Done!')\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def print_data (to_print,sample,info=False,wrapper=False):\n",
    "    \n",
    "    if wrapper:\n",
    "        \n",
    "        tmp1=[]\n",
    "        tmp2=[]\n",
    "        for elem in to_print:\n",
    "            \n",
    "            elem = np.asarray(elem)\n",
    "            tmp1.append(elem[0].reshape(28,28))\n",
    "            tmp2.append(elem[1])\n",
    "        \n",
    "        to_print=[tmp1,tmp2]\n",
    "        \n",
    "    img = to_print[0][sample]\n",
    "    print('\\nCorresponding picture: ')    \n",
    "    img = np.array(img,dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.imshow(pixels,cmap='Greys')\n",
    "    plt.show()\n",
    "    if info:\n",
    "        \n",
    "        digit = to_print[1][sample]\n",
    "        print('Corresponding digit: ',digit,'\\n') \n",
    "        \n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "def sigmoid_prime(z):\n",
    "        \n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# Classes\n",
    "\n",
    "class Network (object):\n",
    "\n",
    "    def __init__(self,sizes):\n",
    "        \n",
    "        self.num_layer = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "    \n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \n",
    "        for b,w in zip(self.biases,self.weights): \n",
    "            \n",
    "            a = self.sigmoid(np.dot(w,a)+b)\n",
    "    \n",
    "        return a  \n",
    "    \n",
    "    def SGD (self, training_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "    \n",
    "        print('Training...')\n",
    "        start = time.time()\n",
    "        if test_data:\n",
    "            \n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                \n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                \n",
    "                print(\"Epoch {0}: {1}/{2}\".format(j, self.fast_eval(test_data),n_test))\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                print(\"Epoch {0}: complete\".format(j))\n",
    "        stop = time.time()\n",
    "        print(\"Done!\\n\\nLearning time: \",(stop-start)//60,\" m \", (stop-start)-60*((stop-start)//60),\" s\\n\")\n",
    "        \n",
    "    def cost_derivative(self,output_a,y):\n",
    "        \n",
    "        return (output_a - y)\n",
    "    \n",
    "    def sigmoid_prime(self,z):\n",
    "        \n",
    "        return sigmoid(z)*(1-sigmoid(z))  \n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "    \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in mini_batch:\n",
    "            \n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb + dnb for nb,dnb in zip(nabla_b,delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw,dnw in zip(nabla_w,delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w - (eta/len(mini_batch))*nw for w,nw in zip(self.weights,nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb for b,nb in zip(self.biases,nabla_b)]\n",
    "    \n",
    "\n",
    "    def backprop(self,x,y):\n",
    "    \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "        \n",
    "            z = np.dot(w,activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "    \n",
    "        delta = self.cost_derivative(activations[-1],y)*self.sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta,activations[-2].transpose())\n",
    "        for l in range(2,self.num_layer):\n",
    "        \n",
    "            delta = np.dot(self.weights[-l+1].transpose(),delta)*self.sigmoid_prime(zs[-l])\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta,activations[-1-l].transpose())\n",
    "    \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def prediction(self,x,real_out=False):\n",
    "        \n",
    "        pred = self.feedforward(x)\n",
    "        if real_out:\n",
    "            \n",
    "            return (np.argmax(pred),pred)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return np.argmax(pred)\n",
    "    \n",
    "    def fast_eval(self,test_data):\n",
    "    \n",
    "        test_result = [(self.prediction(x),y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in test_result)\n",
    "\n",
    "    def evaluate(self, test_data,get_result=True,info=False,get_info=False):\n",
    "    \n",
    "        test_result = [(self.prediction(x),y) for (x,y) in test_data]\n",
    "        dim_dataset = len(test_data)\n",
    "        ev = sum(int(x == y) for (x,y) in test_result)\n",
    "        ev_perc = ev/dim_dataset*100\n",
    "        if get_result:\n",
    "            \n",
    "            print('\\nLevel of accuracy: ', ev_perc,'%')\n",
    "        \n",
    "        if info:\n",
    "            \n",
    "            print('\\nDimension of the testing data set: {0}\\\n",
    "                   \\nNumber of errors: {1}\\\n",
    "                   \\nError probability: {2}'.format(dim_dataset,\n",
    "                                                     dim_dataset-ev,\n",
    "                                                     (dim_dataset-ev)/dim_dataset))\n",
    "        if get_info:\n",
    "            \n",
    "            return (dim_dataset,ev,ev_perc,dim_dataset-ev,(dim_dataset-ev)/dim_dataset)\n",
    "\n",
    "    def save_learning(self,filename='saved'):\n",
    "        \n",
    "        np.save(str(filename)+'_b.npy',self.biases)\n",
    "        np.save(str(filename)+'_w.npy',self.weights)\n",
    "        print('Network\\'s weights and biases saved in {0}_w.npy and {0}_w.npy, respectively.'.format(str(filename)))\n",
    "        \n",
    "    def load_learning(self,name_b='saved_b.npy',name_w='saved_w.npy'):\n",
    "        \n",
    "        self.biases = np.load(str(name_b))\n",
    "        self.weights = np.load(str(name_w))\n",
    "        print('Network\\'s biases and weights loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example, consider the code below which performs the following tasks:\n",
    "\n",
    "1. Open the NIMST database;\n",
    "2. Print a digit and the corresponding value of the MNIST database;\n",
    "3. Create a nn with 784 (the number of pixel of a 28x28 picture of a digit in the MNIST database) input neurons,\n",
    "   50 hidden neurons and 10 output neurons (the ten digits 0,...,9);\n",
    "4. Train the nn and save the learning;\n",
    "5. Evaluate the accuracy of the network;\n",
    "6. Use the trained network to perfom a predition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database...\n",
      "Done!\n",
      "\n",
      "Corresponding picture: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corresponding digit:  1 \n",
      "\n",
      "Training...\n",
      "Epoch 0: complete\n",
      "Epoch 1: complete\n",
      "Epoch 2: complete\n",
      "Epoch 3: complete\n",
      "Epoch 4: complete\n",
      "Epoch 5: complete\n",
      "Epoch 6: complete\n",
      "Epoch 7: complete\n",
      "Epoch 8: complete\n",
      "Epoch 9: complete\n",
      "Epoch 10: complete\n",
      "Epoch 11: complete\n",
      "Epoch 12: complete\n",
      "Epoch 13: complete\n",
      "Epoch 14: complete\n",
      "Epoch 15: complete\n",
      "Epoch 16: complete\n",
      "Epoch 17: complete\n",
      "Epoch 18: complete\n",
      "Epoch 19: complete\n",
      "Epoch 20: complete\n",
      "Epoch 21: complete\n",
      "Epoch 22: complete\n",
      "Epoch 23: complete\n",
      "Epoch 24: complete\n",
      "Epoch 25: complete\n",
      "Epoch 26: complete\n",
      "Epoch 27: complete\n",
      "Epoch 28: complete\n",
      "Epoch 29: complete\n",
      "Done!\n",
      "\n",
      "Learning time:  4.0  m  16.031563997268677  s\n",
      "\n",
      "Network's weights and biases saved in saved_w.npy and saved_w.npy, respectively.\n",
      "\n",
      "Level of accuracy:  95.75 %\n",
      "\n",
      "Dimension of the testing data set: 10000                   \n",
      "Number of errors: 425                   \n",
      "Error probability: 0.0425\n",
      "\n",
      "Corresponding picture: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified value: 6\n",
      "Prediction: 6\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "# 2\n",
    "sample_print = 49\n",
    "print_data(test_data,sample_print,info=True,wrapper=True) \n",
    "\n",
    "#3\n",
    "nn = Network([784,50,10])\n",
    "\n",
    "#4\n",
    "nn.SGD(training_data,30,10,3.0)\n",
    "nn.save_learning()\n",
    "\n",
    "#5\n",
    "nn.evaluate(test_data,info=True)\n",
    "\n",
    "#6\n",
    "sample=21 \n",
    "print_data(test_data,sample,info=False,wrapper=True)\n",
    "print(\"\\nClassified value: {0}\\nPrediction: {1}\".format(test_data[sample][1],nn.prediction(test_data[sample][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we exlplain any part of the code showed above. \n",
    "\n",
    "Let us start with the function ```load_data``` which simply open the ```mnist.pkl.gz``` file and load the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    file = gzip.open('mnist.pkl.gz','rb')\n",
    "    train_set, valid_set, test_set = pkl.load(file,encoding='latin1') \n",
    "    file.close()\n",
    "    return(train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python3 the load function of the _'_pickle'_ library need ```encoding='latin1'```. Let us see how the loaded data lool like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample values:  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01171875 0.0703125  0.0703125  0.0703125\n",
      " 0.4921875  0.53125    0.68359375 0.1015625  0.6484375  0.99609375\n",
      " 0.96484375 0.49609375 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1171875  0.140625   0.3671875  0.6015625\n",
      " 0.6640625  0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.87890625 0.671875   0.98828125 0.9453125  0.76171875 0.25\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19140625\n",
      " 0.9296875  0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.98046875 0.36328125 0.3203125\n",
      " 0.3203125  0.21875    0.15234375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0703125  0.85546875 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.98828125 0.7734375  0.7109375\n",
      " 0.96484375 0.94140625 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3125     0.609375   0.41796875 0.98828125\n",
      " 0.98828125 0.80078125 0.04296875 0.         0.16796875 0.6015625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0546875  0.00390625 0.6015625  0.98828125 0.3515625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54296875 0.98828125 0.7421875  0.0078125  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04296875\n",
      " 0.7421875  0.98828125 0.2734375  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13671875 0.94140625\n",
      " 0.87890625 0.625      0.421875   0.00390625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31640625 0.9375     0.98828125\n",
      " 0.98828125 0.46484375 0.09765625 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17578125 0.7265625  0.98828125 0.98828125\n",
      " 0.5859375  0.10546875 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0625     0.36328125 0.984375   0.98828125 0.73046875\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97265625 0.98828125 0.97265625 0.25       0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1796875  0.5078125  0.71484375 0.98828125\n",
      " 0.98828125 0.80859375 0.0078125  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15234375 0.578125\n",
      " 0.89453125 0.98828125 0.98828125 0.98828125 0.9765625  0.7109375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09375    0.4453125  0.86328125 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.78515625 0.3046875  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.08984375 0.2578125  0.83203125 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.7734375  0.31640625 0.0078125\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0703125  0.66796875\n",
      " 0.85546875 0.98828125 0.98828125 0.98828125 0.98828125 0.76171875\n",
      " 0.3125     0.03515625 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21484375 0.671875   0.8828125  0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.953125   0.51953125 0.04296875 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53125    0.98828125\n",
      " 0.98828125 0.98828125 0.828125   0.52734375 0.515625   0.0625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "\n",
      "Corresponding picture: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample digit: 5\n"
     ]
    }
   ],
   "source": [
    "ts=load_data()[0] # load_data returns 3 objects and we want only the first.\n",
    "sample=0\n",
    "print('Sample values: ', ts[0][sample])\n",
    "print_data(ts,sample)\n",
    "print('Sample digit:', ts[1][sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the MNIST database contains two items.\n",
    "\n",
    "- The first item is an array of numbers 28x28 (=784 pixels) beween 0 and 1. 1 corresponds to the black color and 0 to the white color, while intermediate numbers correspondes to diffenrent kind of the grey. This can be understood comapring the conent of ```ts[0][sample]``` and the picture plotted, by using ```print_data``` (see later). \n",
    "- The second item contain the digit assoicated to the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can easly check that the objects returned by the function are of type tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples are unchangeable objects in Phyton, and for this reason it is better to convert the data into a more flexible format as a list. The function ```load_data_wrapper``` performs this task returning lists as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wrapper():\n",
    "    \n",
    "    print('Loading database...')\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    print ('Done!')\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python3 the ```zip``` function has no ```len()```. The dimension of the object returned by this function is used in the rest of the code. For this reason one need to force zip to evaluate. That is why we use ```list(zip(...))``` to assign the outputs of this function.\n",
    "\n",
    "The function ```print_data``` can be used to visualize the content of the MNIST database. In particular:\n",
    "\n",
    "- to_print = name of the object containing the sample (MNIST database) to be printed;\n",
    "- sample = sample to be printed;\n",
    "- info = set it equal to 'True' to visualize the digit assoicated to the chosen sample (default value 'False');\n",
    "- wrapper = set it equal to 'True' if the object to print come from a ```load_data_wrapper``` (default value 'False')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data (to_print,sample,info=False,wrapper=False):\n",
    "    \n",
    "    if wrapper:\n",
    "        \n",
    "        tmp1=[]\n",
    "        tmp2=[]\n",
    "        for elem in to_print:\n",
    "            \n",
    "            elem = np.asarray(elem)\n",
    "            tmp1.append(elem[0].reshape(28,28))\n",
    "            tmp2.append(elem[1])\n",
    "        \n",
    "        to_print=[tmp1,tmp2]\n",
    "        \n",
    "    img = to_print[0][sample]\n",
    "    print('\\nCorresponding picture: ')    \n",
    "    img = np.array(img, dtype='float')\n",
    "    pixels = img.reshape((28, 28))\n",
    "    plt.imshow(pixels,cmap='Greys')\n",
    "    plt.show()\n",
    "    if info:\n",
    "        \n",
    "        digit = to_print[1][sample]\n",
    "        print('Corresponding digit: ',digit,'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions prensented till are not core directly related to the nn. The nn is implemented defining the class ```Network```. To create a nn with an input layer with N_i input, m hidden layer with N_1,...,N_m neurons and an ouput layer with N_o neurons, we simply write\n",
    "```\n",
    "network_name = Network([N_i,N_1,...N_m,N_o])\n",
    "```\n",
    "The object ```network_name``` create is the desidered nn. To train, use, evaluate,... the network we can use the functions contained in this class, which we will analyse below.\n",
    "\n",
    "We begin with the initializer ```__init__()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network (object):\n",
    "\n",
    "    def __init__(self,sizes):\n",
    "        \n",
    "        self.num_layer = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input of the form ```[N_i,N_1,...,N_m,N_o]```, it initializes randomly all the coefficinets of all the weights matrices of the network. The same is done for all components of all the biases vectors. Those two objects containins all the learning information of the net.\n",
    "\n",
    "The activation function of all the neurons of the network is the sigmoid function $\\sigma(x)$, namely\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "The method below implements the activation function used by the neurons of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoid(self,z):\n",
    "    \n",
    "        return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all the weights matrices and all the biases vector of the network, given an input we can compute the output of the network. This task is performed by the ```feedforward``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def feedforward(self, a):\n",
    "        \n",
    "        for b,w in zip(self.biases,self.weights): \n",
    "            \n",
    "            a = sigmoid(np.dot(w,a)+b)\n",
    "    \n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this function works can be easly understood. According to the sigmoid neuron model (the basic building block of this kind of nn) the input of a neuron are combined together as a linera combination, then the baises is applied and finally one applys the acivation function $\\sigma$ which gives the neuron output. The output of the $k$-th neuron is thus given by\n",
    "\n",
    "$$\n",
    "    o_k = \\sigma\\left( \\sum_j w_{kj}x_j + b_k\\right),\n",
    "$$\n",
    "\n",
    "where $x_j$ are the input of the neurons, $w_{kj}$ the weights, $b_k$ the biases and $o_k$ is the output. To compute the outcome of the network, we need to compute the output of each neuron of each layer till we reach the output layer. This is exactly what the ```for``` cycle does. \n",
    "\n",
    "Now we discuss the key part of a neural network, namely the algorithm responsable for the network training. Let us see how the network learn how to reproduce given some input $x$ a desidered output $y(x)$.\n",
    "\n",
    "1. Given an input $x$ and initializing the network in some way, the output of the network $o$ can be computed. It is a function of the input and on the weights $w=\\{w_{ki}\\}$ and biases $b=\\{b_k\\}$ used in the network initialization, namely $o = o(x,w,b)$.\n",
    "2. Suppose that $y$ is the desidered result of our nn. In general, since the network initialization is arbitrary, $y$ and $o$ are different. Let us call $C(y,o)$ a measure quantifying the error we make with our nn. $C$ is tipically called cost function. It is reasonable to require that such measure is always positive, since it would be hard to link an error with a quantity that may be both positive and negative.\n",
    "3. The idea beyond learing is simple: find the parameters $w$ and $b$ such that $C(y,o)$ is minimum. Find a way to minimize the gradient is the key step of the learning procedure and we will discuss how to perform it in the remaing part of this section. Let $w'$ and $b'$ be the parameters minimizing the cost function, then the output of the trained network will have a minimum error.\n",
    "\n",
    "Note that this is __supervised learning__: we train the nn passing to it a series of examples, from which the network learn by \"trial and error\". Let us specify the cost function we use. A possible choice is the following:\n",
    "\n",
    "$$ \n",
    "    C(w,b) := \\frac{1}{n}\\sum_{i=1}^n \\| o(x_i,w,b) - y(x_i)\\|^2,\n",
    "$$\n",
    "\n",
    "where $\\|\\cdot\\|$ is the euclidean norm, $n$ the total number of training example while $x_i$, for $i=1,\\cdots,n$ are the training example, $o(x_i,w,b)$ is the output of the network and $y(x_i)$ is the desidered output associated to the input $x_i$. Note that this cost function can be interpreted as the mean square error between the network output and the desidered output.\n",
    "\n",
    "To find the minimun of the cost function, a popular method is the __gradient descent__. This method is essentially based on the observation (coming from Analysis) that a function $f$ tends to decrese its value if one moves a bit in the direction given by $-\\nabla_x f$. More precisely, defining\n",
    "\n",
    "$$\n",
    "    x_1 = x_0 - \\eta \\nabla_x f(x_0)\n",
    "$$\n",
    "\n",
    "for $\\eta \\in \\mathbb{R}^+$ sufficiently small, then $f(x_0) > f(x_1)$. Reiterating this procedure we are able find a sequence of value $x_0,\\cdots,x_n$ such that $f(x_0) > f(x_1) > \\cdots > f(x_n)$. Such sequence of values clearly tends to a __local__ minimum of the function $f$. Note that this procedure is not able to find a global minimum in general. This because the sequence of values may point to some local minimum and remain \"trapped\" there. \n",
    "\n",
    "To avoid this problem in a simple and clever way, is to introduce some randomness in the procedure explained above. More precisely, we observe that the cost function chosen is given by\n",
    "\n",
    "$$\n",
    "    C(w,b) = \\frac{1}{n}\\sum_{i=1}^n C_i(w,b)\n",
    "$$\n",
    "\n",
    "(note that the index $i$ corresponds to the input $x_i$ from which the network output can be conputed and some desired output $y_i = y(x_i)$ associated). Hence if we apply the gradient descent prescription, we have to update repeatedly the parametes of as follow\n",
    "\n",
    "\\begin{align}\n",
    "         w' &= w - \\eta \\nabla_w C(w,b) \\\\\n",
    "            &= w - \\frac{\\eta}{n}\\sum_{i=1}^n \\nabla_w C_i(w,b)\n",
    "\\end{align}\n",
    "Similar expression hold for $b$. In the framework of learning teory the paramether $\\eta$ is called __learning rate__ and has to be fixed doing some experimentation. We can see that the updated coefficients are given by an empircal average over all the training dataset. Since the training dataset is typically very large, to make the algorithm more efficient instead of perform the empircal average over the whole training dataset, one can perfom the average over a smaller number of examples $m << n$. This means that we are performing the following approximation\n",
    "\n",
    "$$\n",
    "    \\frac{1}{n}\\sum_{i=1}^n \\nabla_w C_i(w,b) \\approx \\frac{1}{m}\\sum_{j=1}^m \\nabla_w C_j(w,b)\n",
    "$$\n",
    "The smaller number of examples is called __mini-batch__ and $m$ is called mini-batch size. To avoid the creation of artificial paths in the input data due to mini-batchs the creation, we may introduce randomness during the mini-batch creation. Practically this is done perfomrming a random shuffling of the training dataset before the mini-batchs creation. At this point we have a stochastic algorithm approximating the gradient descent algorithm seen before. Such algorithm is sometime called __mini-batch gradient descent__. The update rule for the weights and the biases when the mini-batch has size $m$ are given by\n",
    "\n",
    "\\begin{align}\n",
    "    w \\rightarrow w' = w - \\frac{\\eta_1}{m}\\sum_{i=1}^m \\nabla_w C_i(w,b), \\\\\n",
    "    b \\rightarrow b' = b - \\frac{\\eta_2}{m}\\sum_{i=1}^m \\nabla_b C_i(w,b)\n",
    "\\end{align}\n",
    "\n",
    "(everything we did for the weights $w$ can be repeated ).The extreme version of this algorithm is called __stochastic gradient descent__, in which the mini-batch size is $m=1$. In this case the update rule is simply\n",
    "\n",
    "\\begin{align}\n",
    "    w \\rightarrow w' = w - \\eta_1\\nabla_w C_i(w,b), \\\\\n",
    "    b \\rightarrow b' = b - \\eta_2\\nabla_b C_i(w,b).\n",
    "\\end{align}\n",
    "\n",
    "In this case one speack of __on-line learning__. Once that the net training is terminated for a mini-batch, one say that the nn completed a __epoch__ of training. Since we are using mini-batchs which sample randomly the training dataset, we can repreat this operation many time, i.e. train the network for different epochs. This reasonably improve the network accuracy, but practically this operation means that we are using more time to train the network.\n",
    "\n",
    "In the code presented above, the learing is done using a mini-batch gradient descent algorithm, implemented in the function ```SGD```. The entries of this function are all self explaining except the last:\n",
    "\n",
    "* test_data = passing a test set of data in order to evaluate the performace of the algorithm after each epoch (default value 'None').\n",
    "\n",
    "This functionality is useful to understand the learning performance of the algorithm. Note that we did not compute the derivative of the cost function with respect to the parameters. This task is performed by specific functions, ```update_mini_batch``` and ```backprop```, which we will analyse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def SGD (self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \n",
    "        print('Training...')\n",
    "        start = time.time()\n",
    "        if test_data:\n",
    "            \n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                \n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                \n",
    "                print(\"Epoch {0}: {1}/{2}\".format(j, self.fast_eval(test_data),n_test))\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                print(\"Epoch {0}: complete\".format(j))\n",
    "        stop = time.time()\n",
    "        print(\"Done!\\n\\nLearning time: \",(stop-start)//60,\" m \", (stop-start)-60*((stop-start)//60),\" s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss how to compute the derivative of the cost function with respect to the network parameters. From the previous equations we can easly understand that the key quantities to be computed are $\\nabla_w C_i(w,b)$ and $\\nabla_b C_i(w,b)$. It turns out to be useful to introduce the quantity\n",
    "\n",
    "$$\n",
    "    z_k^s = \\sum_i w_{ki}^sx_i^s + b_k^s \n",
    "$$\n",
    "\n",
    "which is called __weighted input__ of the neurons in the layer $s$. Note that according to the notation used, given a quantity the superscript letter is always associated to the network layer. Note also that it can be rewritten in a vectortial form using the biases vector and the weights matrix. Let us consider gradient of the cost funation with respect to the bias the last first. By using the chain rule for the $k$-th bias of the layer $s$ we can write that\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d }{d b_k^s}C_i(w,b) &= \\sum_j \\frac{d}{d z_j^s}C_i(w,b)\\frac{dz_j^s}{d b_k^s} \\\\\n",
    "                             &= \\sum_j \\frac{d}{d z_j^s}C_i(w,b) \\delta_{jk} = \\frac{d}{d z_k^s}C_i(w,b).\n",
    "\\end{align}\n",
    "\n",
    "The quantity in the RHS will play a special role, and for this reason we introduce the following symbol\n",
    "\n",
    "$$\n",
    "    \\Delta_{k}^s(i) := \\frac{d}{d z_k^s}C_i(w,b).\n",
    "$$\n",
    "\n",
    "This quantity can be intepretted as the network response to a change of the weighted input. Similarly for the $(k,j)$-th weight in the layer $s$, by applying the chain rule we obtain\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d }{d w_{kj}^s}C_i(w,b) &= \\sum_h \\frac{d}{d z_h^s}C_i(w,b)\\frac{dz_h^s}{d w_{kj}^s} \\\\\n",
    "    &= \\sum_h \\Delta_{k}^s(i) \\sum_t \\frac{d w_{ht}^s}{d w_{kj}^s} x_t^s \\\\\n",
    "    &= \\sum_{h,t} \\Delta_{k}^s(i) x_t^s \\delta_{hk}\\delta_{jt} \\\\\n",
    "    &= \\Delta_{k}^s(i) x_j^s = \\Delta_{k}^s(i) o_j^{s-1}\n",
    "\\end{align}\n",
    "\n",
    "where $o_j^{s-1}$ is the output of the $j$-th neuron of the $s-1$ layer (and so it is by construction equal to the $j$-th input of the of the $k$-th neuron in the $s$-th layer of the net). Summarizing we found the following two equations for the gradients\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d }{d b_k^s}C_i(w,b) &= \\Delta_{k}^s(i) \\\\\n",
    "    \\frac{d }{d w_{kj}^s}C_i(w,b) &= \\Delta_{k}^s(i) o_j^{s-1}.\n",
    "\\end{align}\n",
    "\n",
    "Since the function $o_j^{s-1}$ can be easly computed, the central quantity we need to find to compute the gradients is $\\Delta_{k}^s(i)$. Note that the above equations are valid for any layer $s$.\n",
    "\n",
    "Cosinder now the quanity $\\Delta_{k}^s(i)$ for the last layer of the net, i.e. $s=m$. In this case, from the cost function chosen and applying the chain rule we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\Delta_{k}^m(i) &= \\frac{d}{d z_k^m}C_i(w,b) = \\frac{d}{d z_k^m} \\| o^m(x_i,w,b) - y_h(x_i) \\|^2 \\\\\n",
    "                    &= \\frac{d}{d z_k^m} \\sum_h (o_h^m(x_i,w,b) - y_h(x_i))^2 \\\\\n",
    "                    &= \\sum_{h,t}\\frac{d}{d o_t^m} (o_h^m(x_i,w,b) - y_h(x_i))^2 \\frac{d o_t^s}{d z_k^m} \\\\\n",
    "                    &= 2\\sum_{h,t} (o_h^m(x_i,w,b) - y_h(x_i))\\delta_{h,t} \\delta_{t,k}\\sigma'_k(z_t^m) \\\\\n",
    "                    &= 2(o_k^m(x_i,w,b) - y_k(x_i))\\sigma'_k(z_t^m)\n",
    "\\end{align}\n",
    "\n",
    "The factor 2 is not important, since it can be always reabsorbed rescaling the learning rate. For this reason we will omit it. Summarizing for the last laye of the nework we found that \n",
    "\n",
    "$$\n",
    "    \\Delta_{k}^m(i) = (o_k^m(x_i,w,b) - y_k(x_i))\\sigma'_k(z_t^m)\n",
    "$$\n",
    "\n",
    "Note that this equation holds only for the last layer, and it cannot be derived for the layers $s \\neq m$. The first term is simply the derivative of the ($i$-th component of the) cost function with respect to the output of the $k$-th neuron in the last layer. In the code, this is implemented by the method ```cost_derivative```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def cost_derivative(self,output_a,y):\n",
    "        \n",
    "        return (output_a - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term in $\\Delta_{k}^m(i)$ is just the derivative of the neuron activation function. The derivative of a sigmoind activation function has a really nice property: it can be computed enterely in terms of the sigmoind function itself. Indeed, it is not difficult to prove that\n",
    "\n",
    "$$\n",
    "    \\sigma'(x) = \\sigma(x)(1-\\sigma(x)).\n",
    "$$\n",
    "\n",
    "This is implemented in the method ```sigmoid_prime``` of the Network class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoid_prime(self,z):\n",
    "    \n",
    "        return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using these two methods, one can easly compute the $\\Delta_{k}^m(i)$, and so the gradients needed to update the weights and the biases of the network in the _last_ layer. \n",
    "\n",
    "To completely solve the problem we need to find a way to compute $\\Delta_{k}^m(i)$ for $m \\neq s$. This can be done again by simply exploiting the chain rule of calculus. We have\n",
    "\n",
    "\\begin{align}\n",
    "        \\Delta_k^s(i) &= \\frac{d}{d z^s_k}C_i(w,b) = \\sum_j \\frac{d}{d z^{s+1}_j}C_i(w,b)\\frac{d z^{s+1}_j}{d z^s_k}\\\\                       &=\\sum_j \\Delta_j^{s+1}(i)\\frac{d z^{s+1}_j}{d z^s_k}.\n",
    "\\end{align}\n",
    "\n",
    "We can see that in this way we can link $\\Delta$'s of successive layers. Note that since we can compute the $\\Delta$ for the last layer $(s = m)$, we can use the above relation to compute all the $\\Delta$'s in the network __proceeding backward__, from the output layer till the input later. The only thing that we need to do is to compute the last derivative in the RHS of the above expression. Given the weighted input of a neuron in the $s+1$ layer we can write\n",
    "\n",
    "\\begin{align}\n",
    "    z^{s+1}_j &= \\sum_h w_{jh}^{s+1}x_h^{s+1} + b_h^{s+1} \\\\\n",
    "              &= \\sum_h w_{jh}^{s+1}\\sigma_h(z^s) + b_h^{s+1},\n",
    "\\end{align}\n",
    "\n",
    "from which follow that\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d z^{s+1}_j}{d z^s_k} &= \\sum_h w_{jh}^{s+1}\\frac{d \\sigma_h(z^s)}{d z^s_k} \\\\\n",
    "                                &= \\sum_h w_{jh}^{s+1}\\delta_{h,k}\\sigma'(z^s) = w^{s+1}_{jk}\\sigma'(z^s)                              \n",
    "\\end{align}\n",
    "\n",
    "Hence we can write that\n",
    "\n",
    "$$\n",
    "    \\Delta_k^s(i) = \\sum_j \\Delta_j^{s+1}(i)w^{s+1}_{jk}\\sigma'(z^s)\n",
    "$$\n",
    "\n",
    "With this last equation we found a way to compute the derivative of the cost function needed in the parameters update of the gradient descent procedure.\n",
    "\n",
    "\n",
    "Let us now summarize how we can approximate the gradient of the cost function $C(w,b)$ according with the discussion done till now. We recall the the gradient of $C(w,b)$ is approximated averaging over a mini-batch the gradient of the componets $C_i(w,b) = \\| o(x_i,w,b) - y(x_i) \\|^2$. Once that these gradients are found we can determine how to update the paramters in the mini-batch gradient descent algorithm. The steps to perform this task are:\n",
    "\n",
    "1. Compute the $\\Delta(i)$ in the last layer of the network, i.e. $\\Delta_{k}^m(i)$. As we saw, this can be computed from the cost function and the acrivation function derivatives, by using\n",
    "$$\n",
    "    \\Delta_{k}^m(i) = (o_k^m(x_i,w,b) - y_k(x_i))\\sigma'_k(z_t^m).\n",
    "$$\n",
    "\n",
    "\n",
    "2.  Compute the gradient of the cost function with respect the weights and the biases in the last layer of the network according to the formula\n",
    "\\begin{align}\n",
    "    \\frac{d }{d b_k^m}C_i(w,b) &= \\Delta_{k}^m(i), \\\\\n",
    "    \\frac{d }{d w_{kj}^m}C_i(w,b) &= \\Delta_{k}^m(i) o_j^{m-1}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "3. Compute the $\\Delta(i)$ in all the remaining layers of the network, $s = m-1,\\cdots,1$ using\n",
    "$$\n",
    "    \\Delta_k^s(i) = \\sum_j \\Delta_j^{s+1}(i)w^{s+1}_{jk}\\sigma'(z^s),\n",
    "$$\n",
    "and then for each layer compute\n",
    "\\begin{align}\n",
    "    \\frac{d }{d b_k^s}C_i(w,b) &= \\Delta_{k}^s(i), \\\\\n",
    "    \\frac{d }{d w_{kj}^s}C_i(w,b) &= \\Delta_{k}^s(i) o_j^{s-1}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "4. Average all the derivatives of $C_i(w,b)$ over the mini-batch to approximate the gradient of the cost function.\n",
    "\n",
    "\n",
    "5. Update the coefficients in the mini-batch gradient descent algorithm according to the rule\n",
    "\\begin{align}\n",
    "    w \\rightarrow w' = w - \\frac{\\eta_1}{m}\\sum_{i=1}^m \\nabla_w C_i(w,b), \\\\\n",
    "    b \\rightarrow b' = b - \\frac{\\eta_2}{m}\\sum_{i=1}^m \\nabla_b C_i(w,b).\n",
    "\\end{align}\n",
    "\n",
    "The last two tasks are implemented by the method ```update_mini_batch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def update_mini_batch(self, mini_batch, eta):\n",
    "    \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in mini_batch:\n",
    "            \n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb + dnb for nb,dnb in zip(nabla_b,delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw,dnw in zip(nabla_w,delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w - (eta/len(mini_batch))*nw for w,nw in zip(self.weights,nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb for b,nb in zip(self.biases,nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the derivatives of $C_i$ is perfomrmed by the method ```backprop```, which implements the point 1,2 and 3 of the procedure summarized above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self,x,y):\n",
    "    \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b,w in zip(self.biases,self.weights):\n",
    "        \n",
    "            z = np.dot(w,activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "    \n",
    "        delta = self.cost_derivative(activations[-1],y)*self.sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta,activations[-2].transpose())\n",
    "        for l in range(2,self.num_layer):\n",
    "        \n",
    "            delta = np.dot(self.weights[-l+1].transpose(),delta)*self.sigmoid_prime(zs[-l])\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta,activations[-1-l].transpose())\n",
    "    \n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm computes the gradients of the functions $C_i$ automatically and is called __backproagation__. It is a spacial case of a more general class of algorithm called __automatic differentiation__, which are a set of techniques to compute algorithmically a derivative. It is wort to observe that such a computation is more efficient of a numerical computation of the derivative.\n",
    "\n",
    "The core part of the code (the part allowing the network learning) is now ended. The remaining functions performs tasks useful for a trained network. \n",
    "\n",
    "Once that the network has been trained, one can use it to perform a prediction given some input. This is done by the function ```prediction```. The network classify digits as follows: given an input $x$ the neurons in the output layer having the highest value (sometimes output value of a neuron is called __activation__) determines the digit predicted by the network. For example, suppose the the following list contains the activations of all the output neurons\n",
    "\n",
    "```\n",
    "            [0.13 , 0.54 , 0.20 , 0.01 , 0.31 , 0.04 , 0.36 , 0. 42 , 0.05 , 0.28]\n",
    "```\n",
    "then the digit predicted by the network is ```1```, since 0.54 is the higest activation in the output layer. This can be implemented practically by using the Numpy function ```argmax()```, which returns the postion of the element in the list with the highest value. \n",
    "\n",
    "The function ```prediction``` takes the following arguments\n",
    "\n",
    "* x = input data;\n",
    "* real_out = set it equal to to 'True' to return, in addition to the predicted digit, also the list of activations in in the output layer (default value 'False')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prediction(self,x,real_out=False):\n",
    "        \n",
    "        pred = self.feedforward(x)\n",
    "        if real_out:\n",
    "            \n",
    "            return (np.argmax(pred),pred)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return np.argmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the network performace two functions can be used: ```fast_eval``` and ```evaluate```. The first function return simply the number of correct predictions given a set of input data. It is used in the ```SGD``` method to return in a fast way information regarding the performace of the net at each epoch. \n",
    "\n",
    "The function ```evaluate``` returns more information given a set of input data. It has the following arguments:\n",
    "\n",
    "* test_data = the input data set to be used;\n",
    "* get_result = when set equal to 'True' returns the level of accuracy, i.e. the percentage of correct preditions (default value 'True');\n",
    "* info = if set 'True' prints a series of information intresting for the network accuracy, like the dimension of the test dataset, the number of errors and the error probability (default value 'False');\n",
    "* get_info = if set 'True', returns the dimension of the test dataset, the numer of errors and the error probability (default value 'False')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fast_eval(self,test_data):\n",
    "    \n",
    "        test_result = [(self.prediction(x),y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in test_result)\n",
    "\n",
    "    def evaluate(self, test_data,get_result=True,info=False,get_info=False):\n",
    "    \n",
    "        test_result = [(self.prediction(x),y) for (x,y) in test_data]\n",
    "        dim_dataset = len(test_data)\n",
    "        ev = sum(int(x == y) for (x,y) in test_result)\n",
    "        ev_perc = ev/dim_dataset*100\n",
    "        if get_result:\n",
    "            \n",
    "            print('\\nLevel of accuracy: ', ev_perc,'%')\n",
    "        \n",
    "        if info:\n",
    "            \n",
    "            print('\\nDimension of the testing data set: {0}\\\n",
    "                   \\nNumber of errors: {1}\\\n",
    "                   \\n Error probability: {2}'.format(dim_dataset,\n",
    "                                                     dim_dataset-ev,\n",
    "                                                     (dim_dataset-ev)/dim_dataset))\n",
    "        if get_info:\n",
    "            \n",
    "            return (dim_dataset,ev,ev_perc,dim_dataset-ev,(dim_dataset-ev)/dim_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since learning is an expensive operation, it can be useful to save the paramters of the network to use in a second moment. Saving is possible using the ```save_learning``` method. Weights and biases are saved in two different files of format '.npy'. As input, one can write the name of the file: the weights and biases are saved in the files '[NAME OF THE FILE]\\_w.npy' and '[NAME OF THE FILE]\\_b.npy', respectively. A default name 'saved' is present.\n",
    "\n",
    "Loading can be done with the function ```load_learning```. As argument it takes the name of the file used to define the two files '\\_w.npy' and '\\_b.npy' by the ```save_learning``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_learning(self,filename='saved'):\n",
    "        \n",
    "        np.save(str(filename)+'_b.npy',self.biases)\n",
    "        np.save(str(filename)+'_w.npy',self.weights)\n",
    "        print('Network\\'s weights and biases saved in {0}_w.npy and {0}_w.npy, respectively.'.format(str(filename)))\n",
    "        \n",
    "    def load_learning(self,name_b='saved_b.npy',name_w='saved_w.npy'):\n",
    "        \n",
    "        self.biases = np.load(str(name_b))\n",
    "        self.weights = np.load(str(name_w))\n",
    "        print('Network\\'s biases and weights loaded.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
